{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCWfwdEcKIOX"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -q torch\n",
        "!pip install -U accelerate\n",
        "!pip install -q datasets\n",
        "!pip install -U datasets\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q pynvml psutil\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbS3KX9jJ-6H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
        "import psutil\n",
        "import time\n",
        "import threading\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APmqJfEkKDG7"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZr4t2oOKDXY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "hf_tokenn =\"token\"\n",
        "login(hf_tokenn)\n",
        "model_name = 'meta-llama/Llama-3.2-1B'\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name,load_in_8bit=True,device_map=\"auto\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"ItzGenes/pruned70-llama-1b-KD-Bueno\", torch_dtype=torch.float16)\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"ItzGenes/pruned70-llama-1b-KD-Bueno\", torch_dtype=torch.float16)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ItzGenes/pruned70-llama-1b-KD-Bueno\",load_in_8bit=True,device_map={\"\": \"cuda\"})\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ItzGenes/pruned70-llama-1b-KD-Bueno\")\n",
        "model.eval()\n",
        "#model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "g6fgt3EL5Vs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if \"Linear8bitLt\" in str(type(module)):\n",
        "        print(f\"El módulo '{name}' está en 8 bits: {type(module)}\")\n",
        "        break\n",
        "else:\n",
        "    print(\"No se encontraron módulos en 8 bits.\")"
      ],
      "metadata": {
        "id": "E-GMgy1IYLlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUANT 4Bits"
      ],
      "metadata": {
        "id": "npyUx5koL3pO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7U12-zjmbue"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "hf_tokenn =\"token\"\n",
        "login(hf_tokenn)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_name = 'ItzGenes/pruned50-llama-1b-KD-Bueno'\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": \"cuda\"}\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRUNING"
      ],
      "metadata": {
        "id": "-ucLqMWrLqbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "original_param_count = count_parameters(model)"
      ],
      "metadata": {
        "id": "r0DgXTYW0LXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "\n",
        "  gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
        "  up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
        "  importance_scores = gate_max_abs + up_max_abs\n",
        "  return importance_scores\n",
        "\n",
        "  def prune_neuron_pairs(mlp, prune_percent):\n",
        "\n",
        "    gate_weight = mlp.gate_proj.weight.data.float()\n",
        "    up_weight = mlp.up_proj.weight.data.float()\n",
        "\n",
        "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
        "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
        "\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
        "\n",
        "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
        "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
        "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
        "\n",
        "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k\n"
      ],
      "metadata": {
        "id": "VumK3DGhwB2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    It modifies each mlp layer present in model, to retain only the most\n",
        "    important neurons. Creating new smaller versions of each layer pruned.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model.\n",
        "    \"\"\"\n",
        "    new_intermediate_size = None\n",
        "\n",
        "    #loop for each model layer.\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
        "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
        "        # by accesing layer.mlp.\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
        "\n",
        "        #Replace the Origiginal Layers with Pruned Layers.\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        #new_intermediate_size only needs to be set once\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "    #Update the model config file.\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "E-tcSUpXwWic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_neuron_pairs(mlp, prune_percent):\n",
        "    \"\"\"\n",
        "    Reduces the dimensions of the **gate_proj**,**up_proj**, **down_proj**\n",
        "    layers removing the least important neurons.\n",
        "\n",
        "    Args:\n",
        "    - mlp: Layers to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - new_gate_proj, new_up_proj, new_down_proj:  New pruned layers.\n",
        "    - k: New intermediate size.\n",
        "\n",
        "    \"\"\"\n",
        "    # Extract the weights from the MLP layers\n",
        "    #  these weights are used to calculate each neuron's\n",
        "    #  importance score in the next step.\n",
        "    gate_weight = mlp.gate_proj.weight.data.float()\n",
        "    up_weight = mlp.up_proj.weight.data.float()\n",
        "\n",
        "    #Compute importance stores. Neurons with higher importance scores\n",
        "    # are considered more important and less likely to be pruned.\n",
        "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "    #Store the original number of neurons in the intermediate layer.\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "    #Computes the number of neurons to prune.\n",
        "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
        "    #Calculate the number of neurons to keep. The new intermediate size.\n",
        "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
        "\n",
        "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
        "\n",
        "    #Select the neuros to keep, by obtaining the indices to keep.\n",
        "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    #create the new layers\n",
        "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
        "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
        "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
        "\n",
        "    #copy weights to the new layers.\n",
        "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "    #return new layers and intermediate size.\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k"
      ],
      "metadata": {
        "id": "zw_HlnE1znVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_percent = 0.7  # Prune 20% of neurons\n",
        "model = update_model(model, prune_percent)"
      ],
      "metadata": {
        "id": "VNCliSCVwZ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculate the number of parameters\n",
        "\n",
        "pruned_param_count = count_parameters(model)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "z-69FcFmwePi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "In0FQsAIwkw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_name = 'pruned70-llama-1b-32'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "iXbzmHFIwmTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model_name, private=True)\n",
        "tokenizer.push_to_hub(new_model_name)"
      ],
      "metadata": {
        "id": "KOQW5e4qwuYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING\n"
      ],
      "metadata": {
        "id": "z7jyZXuTLxpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_LHxvJdM1g0"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"c4\", \"en\", split=\"validation\", streaming=True)\n",
        "small_subset = []\n",
        "for i, example in enumerate(dataset):\n",
        "    small_subset.append(example)\n",
        "    if i >= 3000:\n",
        "        break\n",
        "\n",
        "print(f\"Usando {len(small_subset)} ejemplos en streaming.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1slHm-FKLtB"
      },
      "outputs": [],
      "source": [
        "\n",
        "stride = 512\n",
        "max_length=5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFJMcdXpKcG8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def text_nll(text: str):\n",
        "    encodings = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=5000).to(device)\n",
        "    input_ids = encodings[\"input_ids\"].to(device)\n",
        "    seq_len = input_ids.size(1)\n",
        "\n",
        "    nll_sum = 0.0\n",
        "    n_tokens = 0\n",
        "    prev_end_loc = 0\n",
        "\n",
        "    for begin_loc in range(0, seq_len, stride):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "\n",
        "        chunk_ids = input_ids[:, begin_loc:end_loc]\n",
        "        target_ids = chunk_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(chunk_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs.loss\n",
        "\n",
        "        num_valid = (target_ids != -100).sum().item()\n",
        "        batch_sz = target_ids.size(0)\n",
        "        num_loss_tokens = num_valid #- batch_sz prueba\n",
        "\n",
        "        nll_sum += neg_log_likelihood.item() * num_loss_tokens\n",
        "        n_tokens += num_loss_tokens\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "\n",
        "    return nll_sum, n_tokens\n",
        "\n",
        "total_nll = 0.0\n",
        "total_tokens = 0\n",
        "\n",
        "for ex in tqdm(small_subset, desc=\"Evaluando subset\"):\n",
        "    text = ex[\"text\"]\n",
        "    nll, toks = text_nll(text)\n",
        "    if math.isnan(nll):\n",
        "      print(f\"NLL es NaN — texto: {repr(text[:200])}\")\n",
        "    total_nll += nll\n",
        "    total_tokens += toks\n",
        "    torch.cuda.empty_cache()\n",
        "    #print(f\"text_nll: nll={nll}, toks={toks}\")\n",
        "\n",
        "\n",
        "if total_tokens == 0:\n",
        "    raise ValueError(\"No se evaluaron tokens válidos. Revisa el subset o la lógica del procesamiento.\")\n",
        "print(total_nll)\n",
        "avg_nll = total_nll / total_tokens\n",
        "perplexity = torch.exp(torch.tensor(avg_nll))\n",
        "\n",
        "print(f\"\\n=== RESULTADOS ===\")\n",
        "print(f\"Tokens totales evaluados: {total_tokens}\")\n",
        "print(f\"Pérdida media por token (NLL): {avg_nll:.4f}\")\n",
        "print(f\"Perplexity global: {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1qIupeu_QQY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def accuracy_for_text(text: str):\n",
        "    with torch.no_grad():\n",
        "      enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=5000)\n",
        "      input_ids = enc.input_ids.to(device)\n",
        "      seq_len = input_ids.size(1)\n",
        "\n",
        "      n_tokens = 0\n",
        "      n_correct = 0\n",
        "      prev_end = 0\n",
        "\n",
        "      for start in range(0, seq_len, stride):\n",
        "          end = min(start + max_length, seq_len)\n",
        "          trg_len = end - prev_end\n",
        "\n",
        "          chunk = input_ids[:, start:end]\n",
        "          target = chunk.clone()\n",
        "          target[:, :-trg_len] = -100\n",
        "\n",
        "          outputs = model(chunk, labels=target)\n",
        "          logits = outputs.logits\n",
        "          preds = torch.argmax(logits, dim=-1)\n",
        "          mask = target != -100\n",
        "          correct = (preds == target) & mask\n",
        "\n",
        "          n_correct += correct.sum().item()\n",
        "          n_tokens += mask.sum().item()\n",
        "\n",
        "          prev_end = end\n",
        "          if end == seq_len:\n",
        "              break\n",
        "\n",
        "\n",
        "    return n_tokens, n_correct\n",
        "\n",
        "total_tokens = 0\n",
        "total_correct = 0\n",
        "\n",
        "for ex in tqdm(small_subset, desc=\"Calculando accuracy\"):\n",
        "    text = ex[\"text\"]\n",
        "    toks, correct = accuracy_for_text(text)\n",
        "    total_tokens += toks\n",
        "    total_correct += correct\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "accuracy = total_correct / total_tokens\n",
        "print(f\"\\n=== RESULTADO ===\")\n",
        "print(f\"Tokens evaluados: {total_tokens}\")\n",
        "print(f\"Accuracy por token: {accuracy:.4%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqaAD5jjMtnn"
      },
      "outputs": [],
      "source": [
        "#model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def topk_accuracy(text, k=5, max_length=2048, stride=512):\n",
        "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=None)\n",
        "    input_ids = enc.input_ids.to(device)\n",
        "    seq_len = input_ids.size(1)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for start in range(0, seq_len - 1, stride):\n",
        "        end = min(start + max_length, seq_len)\n",
        "        input_chunk = input_ids[:, start:end]\n",
        "        labels = input_chunk[:, 1:]\n",
        "        inputs = input_chunk[:, :-1]\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs.logits\n",
        "        topk = torch.topk(logits, k=k, dim=-1).indices\n",
        "        match = (topk == labels.unsqueeze(-1)).any(dim=-1)\n",
        "        correct += match.sum().item()\n",
        "        total += match.numel()\n",
        "\n",
        "        if end == seq_len:\n",
        "            break\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "total_correct = 0\n",
        "total_count = 0\n",
        "\n",
        "for ex in tqdm(small_subset, desc=\"Evaluando Top-5 accuracy\"):\n",
        "    text = ex[\"text\"]\n",
        "    correct, total = topk_accuracy(text, k=5)\n",
        "    total_correct += correct\n",
        "    total_count += total\n",
        "\n",
        "top5_acc = total_correct / total_count\n",
        "print(f\"\\nTop-5 Accuracy: {top5_acc:.4%} ({total_correct}/{total_count})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ3XNf6HNrSC"
      },
      "source": [
        "Uso de Memoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG8XPIwJK7eC"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import logging\n",
        "logging.set_verbosity_error()\n",
        "#model.to(device)\n",
        "max_new_tokens = 50\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def memory_for_prompt(text: str) -> float:\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "        _ = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        peak_bytes = torch.cuda.max_memory_allocated(device)\n",
        "        peak_gb = peak_bytes / 1e9\n",
        "        return peak_gb\n",
        "\n",
        "memory_peaks = []\n",
        "\n",
        "for ex in tqdm(small_subset, desc=\"Midiendo memoria por ejemplo\"):\n",
        "    text = ex[\"text\"]\n",
        "    peak = memory_for_prompt(text)\n",
        "    memory_peaks.append(peak)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Calcular métricas finales\n",
        "peak_total = max(memory_peaks)\n",
        "mean_peak = float(np.mean(memory_peaks))\n",
        "std_peak  = float(np.std(memory_peaks))\n",
        "\n",
        "# Imprimir resultados\n",
        "print(f\"\\n=== RESULTADO MEMORIA ===\")\n",
        "print(f\"Pico total observado:     {peak_total:.3f} GB\")\n",
        "print(f\"Media por ejemplo:        {mean_peak:.3f} GB ± {std_peak:.3f} GB (σ)\")\n",
        "print(memory_peaks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLGvUBu8NpWg"
      },
      "source": [
        "`\\Medir latencia + TTFT y TPOT (Time To First Token y Time Per Output Token)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d84gMcxpNXRw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from transformers.utils import logging\n",
        "\n",
        "max_new_tokens = 50\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def timing_for_prompt(text: str):\n",
        "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "    input_ids = enc.input_ids.to(device)\n",
        "\n",
        "    torch.cuda.synchronize(device)\n",
        "    start_ttft = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=1,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    torch.cuda.synchronize(device)\n",
        "    ttft = time.perf_counter() - start_ttft\n",
        "\n",
        "    torch.cuda.synchronize(device)\n",
        "    start_total = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    torch.cuda.synchronize(device)\n",
        "    total_latency = time.perf_counter() - start_total\n",
        "\n",
        "\n",
        "    n_generated = output.size(-1) - input_ids.size(-1)\n",
        "    if n_generated > 1:\n",
        "        tpot = (total_latency - ttft) / (n_generated - 1)\n",
        "    else:\n",
        "        tpot = 0.0\n",
        "\n",
        "    return ttft, total_latency, tpot\n",
        "\n",
        "# Listas para acumular métricas\n",
        "ttfts, totals, tpots = [], [], []\n",
        "\n",
        "# Iterar sobre el subset\n",
        "for ex in tqdm(small_subset, desc=\"Midiendo latencias por ejemplo\"):\n",
        "    text = ex[\"text\"]\n",
        "    ttft, total, tpot = timing_for_prompt(text)\n",
        "    ttfts.append(ttft)\n",
        "    totals.append(total)\n",
        "    tpots.append(tpot)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "metrics = {\n",
        "    \"TTFT (s)\": ttfts,\n",
        "    \"Latencia total (s)\": totals,\n",
        "    \"TPOT (s/token)\": tpots\n",
        "}\n",
        "\n",
        "for name, vals in metrics.items():\n",
        "    peak = max(vals)\n",
        "    mean = float(np.mean(vals))\n",
        "    std  = float(np.std(vals))\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Peak : {peak:.3f} s\")\n",
        "    print(f\"Media: {mean:.3f} s ± {std:.3f} s (σ)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr6tflGBwdln"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(totals)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slbulL9iNtA3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "\n",
        "print(\"¿Usa 4-bit?\", any(isinstance(m, Linear4bit) for m in model.modules()))"
      ],
      "metadata": {
        "id": "yD77Rw2s0xIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "npyUx5koL3pO",
        "-ucLqMWrLqbg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}