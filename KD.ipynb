{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zHApVm41HWq"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q torch\n",
        "!pip install -q datasets\n",
        "!pip install -q sentencepiece  # Required for LLaMA tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJNgRj4M187E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbIyUlXEtbqs"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-z_1Zpg2I6u"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "hf_tokenn =\"token\"\n",
        "login(hf_tokenn)\n",
        "model_name = 'meta-llama/Llama-3.2-1B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_model_name = \"./pruned70-llama-1b-32\"\n",
        "\n",
        "student_model =  AutoModelForCausalLM.from_pretrained(\"ItzGenes/pruned70-llama-1b-32\")"
      ],
      "metadata": {
        "id": "ybEth3LcZn06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n"
      ],
      "metadata": {
        "id": "9gFNU9Rvau3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "import shutil\n",
        "import os\n",
        "from itertools import islice\n",
        "dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
        "dataset = list(islice(dataset, 3000))\n",
        "\n",
        "\n",
        "dataset = Dataset.from_list(dataset)"
      ],
      "metadata": {
        "id": "TXk1EPIWaVxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "sSFK1wsSbL85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIoi2PiwXax1"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    labels = input_ids.copy()\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": labels,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset with progress bar\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Processing examples\",\n",
        "    load_from_cache_file=False\n",
        ")"
      ],
      "metadata": {
        "id": "C8Q5CV6da7Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    tokenized_datasets,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "-MAEPOPObOa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "student_model.to(device)"
      ],
      "metadata": {
        "id": "C41_ffvIbP1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "63oH5KsabXtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "temperature = 2.0\n",
        "alpha = 1\n",
        "\n",
        "accumulation_steps = 8"
      ],
      "metadata": {
        "id": "RBH97fRTbZYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_allocated() / 1e6, \"MB\")"
      ],
      "metadata": {
        "id": "igBOZ12GeD_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#En realidad para KD no hace falta ejecutar los codigos de arriba, estan aqui ya integrados, estan repetidos arriba por motivo de errores que daban anteiormente y hacer pruebas varias\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Processing examples\",\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    tokenized_datasets,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "\n",
        "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
        "\n",
        "num_epochs = 5\n",
        "temperature = 2.0\n",
        "alpha = 1\n",
        "\n",
        "accumulation_steps = 8\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    student_model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            teacher_outputs = model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            teacher_logits = teacher_outputs.logits / temperature\n",
        "\n",
        "        student_outputs = student_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        student_logits = student_outputs.logits\n",
        "\n",
        "\n",
        "        teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
        "\n",
        "        student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "        loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
        "\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(dataloader)):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "XZDK9hNXb6Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qEmvooZycx"
      },
      "source": [
        "#Upload the model to HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Ll_kqe5QzO"
      },
      "outputs": [],
      "source": [
        "new_model_name = 'pruned70-llama-1b-KD-Bueno'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "student_model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LjjsGZV5ZHJ"
      },
      "outputs": [],
      "source": [
        "student_model.push_to_hub(new_model_name, private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xNN-aYa5h9B"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(new_model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}